---
title: "Multi Amplicon - a real world example"
author: "Emanuel Heitlinger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

## 

We first download the sample data set from SRA using a helper scirtp
part of the MultiAmplicon package.

You will need about 2Gb space to store those files if you want to
execute the pipeline below on your local computer and/or compile the
vignette locally.

```{r getData, }
library(SRAdb) 

sqlfile <- "SRAmetadb.sqlite"
if(!file.exists("SRAmetadb.sqlite")) sqlfile <<- getSRAdbFile()
sqlfile <- getSRAdbFile()
sra_con <- dbConnect(SQLite(),sqlfile)

rs <- getSRA(search_terms = '"Intestinal biome sequencing of carnivores"' ,
             sra_con=sra_con, acc_only=TRUE)

runs <- tail(rs$run, n=50)

fs <- getSRAinfo( runs, sra_con, sraType = "sra" )

destDir <- "./download"

if(!file_test("-d", destDir)) dir.create(destDir)

fastqFiles <- list.files(destDir, pattern=".fastq.gz", full.names=TRUE)

if(length(fastqFiles) < 100){
    getSRAfile(runs, sra_con, fileType = 'fastq' , srcType = "ftp",
               destDir=destDir)
    fastqFiles <- list.files(destDir, pattern=".fastq.gz", full.names=TRUE)
}
```

Now we can run our standard workflow. But we have to remember to filte
our files first. This particular sequencing was of quite low quality,
so we have to trim and screen quite harshly.

```{r filter}
fastqF <- grep("_1.fastq.gz", fastqFiles, value = TRUE)
fastqR <- grep("_2.fastq.gz", fastqFiles, value = TRUE)

samples <- gsub("_1.fastq\\.gz", "\\1", basename(fastqF))

filt_path <- "./filtered"
if(!file_test("-d", filt_path)) dir.create(filt_path)

filtFs <- file.path(filt_path, paste0(samples, "_F_filt.fastq.gz"))
names(filtFs) <- samples
filtRs <- file.path(filt_path, paste0(samples, "_R_filt.fastq.gz"))
names(filtRs) <- samples

filter.track <- lapply(seq_along(fastqF),  function (i) {
    filterAndTrim(fastqF[i], filtFs[i], fastqR[i], filtRs[i],
                  truncLen=c(170,170), minLen=c(170,170), 
                  maxN=0, maxEE=2, truncQ=2, 
                  compress=TRUE, verbose=TRUE)
})
```

Now we have our sequencing read input data, only the primers are still
missing.

```{r prep primers}
primer.file <- system.file("extdata", "real_world_primers.csv", package = "MultiAmplicon")

ptable <- read.csv(primer.file, sep=",", header=TRUE)
```
