---
title: "Multi Amplicon - a real world example"
author: "Emanuel Heitlinger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Data preparation (download)

We first download the raw fastq files of the sample data set
[from here](https://svalbard.biologie.hu-berlin.de:443/d/c291b870178f4abc8c59/).
 

This data set was downloaded from NCBI's SRA using the process
described in the
[help file for carnivoreSeqRuns](https://derele.github.io/MultiAmplicon/reference/carnivoreSeqRuns.html).

You will need about 10 Gb disk space to store those files if you want
to execute the pipeline below on your local computer and/or to compile
the vignette locally.

```{r getData}
library(MultiAmplicon)

data(carnivoreSeqRuns)
runs <- tail(carnivoreSeqRuns$run, n=50)

destDir <- "./download"

if(!file_test("-d", destDir)) dir.create(destDir)

fastqFiles <- list.files(destDir, pattern=".fastq.gz", full.names=TRUE)

if(length(fastqFiles) < 100){
    getSRAfile(runs, sra_con, fileType = 'fastq' , srcType = "ftp",
               destDir=destDir)
    fastqFiles <- list.files(destDir, pattern=".fastq.gz", full.names=TRUE)
}
```

## Data preparation (filtering)

Now we can run our standard workflow. But in real world example we
have to quality trim and filter our input files first. This particular
sequencing was of quite low quality, so we have to trim and screen
quite harshly.

```{r filter, message=FALSE}
fastqF <- grep("_1.fastq.gz", fastqFiles, value = TRUE)
fastqR <- grep("_2.fastq.gz", fastqFiles, value = TRUE)

samples <- gsub("_1.fastq\\.gz", "\\1", basename(fastqF))

filt_path <- "./filtered"
if(!file_test("-d", filt_path)) dir.create(filt_path)

filtFs <- file.path(filt_path, paste0(samples, "_F_filt.fastq.gz"))
names(filtFs) <- samples
filtRs <- file.path(filt_path, paste0(samples, "_R_filt.fastq.gz"))
names(filtRs) <- samples

filter.track <- lapply(seq_along(fastqF),  function (i) {
    filterAndTrim(fastqF[i], filtFs[i], fastqR[i], filtRs[i],
                  truncLen=c(170,170), minLen=c(170,170), 
                  maxN=0, maxEE=2, truncQ=2, 
                  compress=TRUE, verbose=TRUE)
})

names(filtFs) <- names(filtRs) <- samples

files <- PairedReadFileSet(filtFs, filtRs)
```

## Data preparation (primer set)

Now we have our sequencing read input data, only the primers are still
missing. The primer set for the above is included in the MultiAmplicon
package.

```{r prep primers}

primer.file <- system.file("extdata", "real_world_primers.csv", package = "MultiAmplicon")

ptable <- read.csv(primer.file, sep=",", header=TRUE, stringsAsFactors=FALSE)

primerF <- ptable[, "TS.SequenceF"]
primerR <- ptable[, "TS.SequenceR"]

names(primerF) <- as.character(ptable[, "corrected.NameF"])
names(primerR) <- as.character(ptable[, "corrected.NameR"])

primers <- PrimerPairsSet(primerF, primerR)
```
## Running the MultiAmplicon pipeline

